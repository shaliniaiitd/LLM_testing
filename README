ğŸš€ LLM Test Automation Framework

A Pytest-based framework for evaluating LLM behavior across quality, consistency, safety, hallucination, and performance.

This repository contains a modular automation framework focused on systematically validating Large Language Models using Python, Pytest, and semantic evaluation techniques.

ğŸ“ Current Project Structure

Since this version of the project only contains tests and utils, here is the accurate layout:

llm_test_automation/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_prompt_quality.py
â”‚   â”œâ”€â”€ test_response_validation.py
â”‚   â”œâ”€â”€ test_consistency.py
â”‚   â”œâ”€â”€ test_hallucination.py
â”‚   â”œâ”€â”€ test_bias_safety.py
â”‚   â””â”€â”€ test_performance.py
â”‚
â””â”€â”€ utils/
    â”œâ”€â”€ llm_client.py             # Unified wrapper for LLM API calls
    â”œâ”€â”€ evaluator.py              # Scoring logic: factuality, coherence, structure
    â”œâ”€â”€ metrics_calculator.py     # Semantic similarity metrics
    â””â”€â”€ logger.py                 # Central logging utilities


(More folders like data/, config/, or reports/ can be added later.)

ğŸ§  What This Framework Tests
1ï¸âƒ£ Response Quality

Coherence

Completeness

Instruction adherence

2ï¸âƒ£ Hallucination Detection

Checks if LLM invents facts

Flags unsupported claims

3ï¸âƒ£ Consistency Testing

Compares outputs across repeated runs

Measures semantic drift

4ï¸âƒ£ Safety, Toxicity & Bias

Ensures model refuses unsafe queries

Detects biased or harmful content

5ï¸âƒ£ Performance & Cost

Latency measurements

Token usage

Retry & timeout mechanisms

âš™ï¸ How to Run
Install dependencies
pip install -r requirements.txt

Set API keys
export OPENAI_API_KEY=YOUR_KEY

Run tests
pytest -q

ğŸ› ï¸ Tech Used

Python 3.10+

Pytest

Sentence Transformers (semantic similarity)

OpenAI API / Anthropic API

Allure (optional for reporting)


Evaluation Libraries (to explore):
â”œâ”€â”€ RAGAS (RAG evaluation)
â”œâ”€â”€ LangChain (for LLM apps)
â”œâ”€â”€ DeepEval (LLM evaluation framework)
â””â”€â”€ TruLens (LLM observability)
